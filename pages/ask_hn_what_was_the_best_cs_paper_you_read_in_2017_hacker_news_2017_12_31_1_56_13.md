<a href="https://news.ycombinator.com/item?id=16035402">https://news.ycombinator.com/item?id=16035402</a><div id="articleHeader"><h1>Ask HN: What was the best CS paper you read in 2017?</h1></div>
        
<table>
            <tbody><tr id="16036077"><td>
            <table>  <tbody><tr>    <td></td><td></td><td><div>
                  Chord: <a href="http://nms.lcs.mit.edu/papers/chord.pdf" target="_blank">http://nms.lcs.mit.edu/papers/chord.pdf</a><p>I think this paper amazing because it solves a complex problem with a simple solution. How do you create a hash function that adjust to the varying number of underlying buckets ? Solution: hash to a circle.
              </p><div>        <p>
                      <u><a href="reply?id=16036077&goto=item%3Fid%3D16035402%2316036077" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
              <tr id="16036038"><td>
            <table>  <tbody><tr>    <td></td><td></td><td><div>
                  <i>seL4: Formal Verification of an OS Kernel</i> (from 2009).<p><a href="https://www.sigops.org/sosp/sosp09/papers/klein-sosp09.pdf" target="_blank">https://www.sigops.org/sosp/sosp09/papers/klein-sosp09.pdf</a></p><p>seL4 is about 9000 LOC. So this gives a good indication of what formal verification (Isabelle/HOL) is currently capable of. seL4 is also quite fast as a result of removing unnecessary checks.</p><p><a href="https://sel4.systems/" target="_blank">https://sel4.systems/</a></p><p>seL4 is a lot smaller than L4 and it's also capability based which L4 isn't. They could have called it L5 or seL5.
              </p><div>        <p>
                      <u><a href="reply?id=16036038&goto=item%3Fid%3D16035402%2316036038" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
              <tr id="16035723"><td>
            <table>  <tbody><tr>    <td></td><td></td><td><div>
                  I highly recommend people give this paper a read. I think it points the way to a radical redesign of fundamental parts of the system stack over the next 5-10 years. If you work in systems and you aren’t thinking about this stuff, you’re about to be lapped.<p>The Case for Learned Index Structures:</p><p><a href="https://arxiv.org/pdf/1712.01208v1.pdf" target="_blank">https://arxiv.org/pdf/1712.01208v1.pdf</a>
              </p><div>        <p>
                      <u><a href="reply?id=16035723&goto=item%3Fid%3D16035402%2316035723" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
        <tr id="16036060"><td>
            <table>  <tbody><tr>    <td><img src="s.gif" width="40" height="1" /></td><td></td><td><div>
                  That was a good read, thanks for sharing this.
              <div>        <p>
                      <u><a href="reply?id=16036060&goto=item%3Fid%3D16035402%2316036060" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
                <tr id="16035593"><td>
            <table>  <tbody><tr>    <td></td><td></td><td><div>
                  It's an old one, but this year I read The Emperor's Old Clothes by C.A.R. Hoare for the first time and it got me started in a Hoare rabbit hole, just a brilliant guy.<p>I read the annotated version on Fermat's Library -
 great source of interesting papers (<a href="http://fermatslibrary.com/s/the-emperors-old-clothes" target="_blank">http://fermatslibrary.com/s/the-emperors-old-clothes</a>).
              </p><div>        <p>
                      <u><a href="reply?id=16035593&goto=item%3Fid%3D16035402%2316035593" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
              
              <tr id="16035493"><td>
            <table>  <tbody><tr>    <td></td><td></td><td><div>
                  The raft consensus algorithm article [1] and the follow up that modifies it slightly with some changes that are useful when implementing it [2]. Both are written very clearly and very focused on actually using the algorithm in practice. Problems that arise when implementing it are discussed and solutions are proposed as well.<p>[1] <a href="https://raft.github.io/raft.pdf" target="_blank">https://raft.github.io/raft.pdf</a></p><p>[2] <a href="http://openlife.cc/system/files/4-modifications-for-Raft-consensus.pdf" target="_blank">http://openlife.cc/system/files/4-modifications-for-Raft-con...</a>
              </p><div>        <p>
                      <u><a href="reply?id=16035493&goto=item%3Fid%3D16035402%2316035493" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
              
              <tr id="16035968"><td>
            <table>  <tbody><tr>    <td></td><td></td><td><div>
                  "Foraging Goes Mobile: Foraging While Debugging on Mobile Devices" (ftp://ftp.cs.orst.edu/pub/burnett/vlhcc17-foraging-mobile.pdf)<p>"Toward Principles for the Design of Navigation Affordances in Code Editors: An Empirical Investigation" (<a href="http://dl.acm.org/authorize?N37917" target="_blank">http://dl.acm.org/authorize?N37917</a>)
              </p><div>        <p>
                      <u><a href="reply?id=16035968&goto=item%3Fid%3D16035402%2316035968" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
              <tr id="16035624"><td>
            <table>  <tbody><tr>    <td></td><td></td><td><div>
                  * What Developers Want and Need from Program Analysis - An Empirical Study<p>* Weird machines, exploitability, and provable
unexploitability</p><p>* What You Corrupt Is Not What You Crash:
Challenges in Fuzzing Embedded Devices
              </p><div>        <p>
                      <u><a href="reply?id=16035624&goto=item%3Fid%3D16035402%2316035624" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
        
                <tr id="16035877"><td>
            <table>  <tbody><tr>    <td></td><td></td><td><div>
                  Does anyone have any suggestions on finding CS papers worth reading beyond those papers listed here?
              <div>        <p>
                      <u><a href="reply?id=16035877&goto=item%3Fid%3D16035402%2316035877" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
        <tr id="16035948"><td>
            <table>  <tbody><tr>    <td><img src="s.gif" width="40" height="1" /></td><td></td><td><div>
                  What topics are you interested in? Following the top ACM and IEEE conferences in that area is a good start!<p>For example, I'm interested in human-computer interaction so I read CHI and UIST papers each year, and for software engineering I read ICSE and FSE.
              </p><div>        <p>
                      <u><a href="reply?id=16035948&goto=item%3Fid%3D16035402%2316035948" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
                <tr id="16035648"><td>
            <table>  <tbody><tr>    <td></td><td></td><td><div>
                  "Memcomputing NP-complete problems in polynomial time using polynomial resources and collective states" <a href="http://advances.sciencemag.org/content/1/6/e1500031.full" target="_blank">http://advances.sciencemag.org/content/1/6/e1500031.full</a>
              <div>        <p>
                      <u><a href="reply?id=16035648&goto=item%3Fid%3D16035402%2316035648" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
        <tr id="16035759"><td>
            <table>  <tbody><tr>    <td><img src="s.gif" width="40" height="1" /></td><td></td><td><div>
                  Could you tell us a bit about the reñevance of this work?
              <div>        <p>
                      <u><a href="reply?id=16035759&goto=item%3Fid%3D16035402%2316035759" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
        <tr id="16036041"><td>
            <table>  <tbody><tr>    <td><img src="s.gif" width="80" height="1" /></td><td></td><td><div>
                  They claim they can actually build things that compute np-complete problems in polynomial time for real.<p>"We show an experimental demonstration of an actual memcomputing architecture that solves the NP-complete version of the subset sum problem in only one step and is composed of a number of memprocessors that scales linearly with the size of the problem. We have fabricated this architecture using standard microelectronic technology so that it can be easily realized in any laboratory setting"</p><p>This is traditionally what people look to quantum computing to solve, but that seems much farther off in practice that the technology described here, at least, as described.</p><p>TL;DR Memcomputing has been shown to have the same power as non-deterministic turing machines. They claim to have made some real ones with promising results. It looks like it's getting commercialized over at <a href="http://memcpu.com/" target="_blank">http://memcpu.com/</a>.</p><p>I'm just summarizing the papers/data, not commenting on it for real, it could all just be snake oil
              </p><div>        <p>
                      <u><a href="reply?id=16036041&goto=item%3Fid%3D16035402%2316036041" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
                  
              
              
              <tr id="16035570"><td>
            <table>  <tbody><tr>    <td></td><td></td><td><div>
                  I read a lot of good ones but I'll answer what was <i>most important</i> rather than best. We're seeing a revival of formal methods ranging from lightweight (TLA+) to heavy. Two problems are in my sight: lack of framework plus consistent, empirical data for evaluating how suitable a given method is for a specific project or company; many who would use lightweight or practical methods will quit because someone convinced them to start with heavy ones. The paper below is a start on the former:<p><a href="https://www.scch.at/de/rse-news/passende-formale-methon?file=files/Dokumente/SCCH_Web/Downloads/Downloads_2015/DownloadBilder/Neu_SCCH-TR-1603_FormalMethodsComparison.pdf" target="_blank">https://www.scch.at/de/rse-news/passende-formale-methon?file...</a></p><p>I think that was a great start on an evalution criteria because its comparisons match what I read in the various experience reports. ASM's and TLA+ coming in the lead on usability but B method on code generation. It will also help to note what projects have been completed or reusable libraries available in each. A person building verified compilers for functional languages might want to learn Isabelle/HOL due to CakeML, doing a new TLS extension learn F* due to miTLS, type-safe assembly learn Coq due to CoqASM, and so on.</p><p>For the other issue, I made a proposal on HN first and then on Lobste.rs about avoiding people quitting early from finding formal methods too overwhelming or useless. I proposed having a default intro or lure that matched the goals of different people to tools with a warning of some's difficulty. It got some interesting responses (below) I'll turn into a mini-essay with links eventually.</p><p><a href="https://lobste.rs/s/n7v658/your_thoughts_on_this_advice_those" target="_blank">https://lobste.rs/s/n7v658/your_thoughts_on_this_advice_thos...</a>
              </p><div>        <p>
                      <u><a href="reply?id=16035570&goto=item%3Fid%3D16035402%2316035570" target="_blank">reply</a></u>
                  
      </p></div></div></td></tr>
      </tbody></table></td></tr>
            </tbody></table>
  